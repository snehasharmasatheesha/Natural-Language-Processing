{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0f444ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('brown')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6f103a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3979640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()#each file is a source will be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "003a289f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', ...]\n"
     ]
    }
   ],
   "source": [
    "macbeth = gutenberg.words('shakespeare-macbeth.txt') # Returns a list of strings\n",
    "print(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93edbb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23140"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b7b83d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a list of list of strings \n",
    "macbeth_sentences = gutenberg.sents(fileids='shakespeare-macbeth.txt') # You can access a specific file with `fileids` argument.\n",
    "macbeth_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07bfb485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 25 26 austen-emma.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "5 28 22 austen-sense.txt\n",
      "4 34 79 bible-kjv.txt\n",
      "5 19 5 blake-poems.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "4 18 12 burgess-busterbrown.txt\n",
      "4 20 13 carroll-alice.txt\n",
      "5 20 12 chesterton-ball.txt\n",
      "5 23 11 chesterton-brown.txt\n",
      "5 18 11 chesterton-thursday.txt\n",
      "4 21 25 edgeworth-parents.txt\n",
      "5 26 15 melville-moby_dick.txt\n",
      "5 52 11 milton-paradise.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n",
      "5 36 12 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "#a program to display other information about each text,\n",
    "#by looping over all the valus of fieldid corresponding to the\n",
    "#gutenberg file\n",
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set([w.lower() for w in gutenberg.words(fileid)]))\n",
    "    print (round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "edf92582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids()[:5]) # First 100 sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef10df1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Thunder and Lightning. Enter three Witches.\n",
      "\n",
      "  1. When shall we three meet againe?\n",
      "In Thunder, Lightning, or in Raine?\n",
      "  2. When the Hurley-burley's done,\n",
      "When the Battaile's lost, and wonne\n",
      "\n",
      "   3. That will be ere the set of Sunne\n",
      "\n",
      "   1. Where the place?\n",
      "  2. Vpon the Heath\n",
      "\n",
      "   3. There to meet with Macbeth\n",
      "\n",
      "   1. I come, Gray-Malkin\n",
      "\n",
      "   All. Padock calls anon: faire is foule, and foule is faire,\n",
      "Houer through the fogge and filthie ayre.\n",
      "\n",
      "Exeunt.\n",
      "\n",
      "\n",
      "Scena Secunda.\n",
      "\n",
      "Alarum within. Enter King Malcome, Donalbaine, Lenox, with\n",
      "attendants,\n",
      "meeting a bleeding Captaine.\n",
      "\n",
      "  King. What bloody man is that? he can report,\n",
      "As seemeth by his plight, of the Reuolt\n",
      "The newest state\n",
      "\n",
      "   Mal. This is the Serieant,\n",
      "Who like a good and hardie Souldier fought\n",
      "'Gainst my Captiuitie: Haile braue friend;\n",
      "Say to the King, the knowledge of the Broyle,\n",
      "As thou didst leaue it\n",
      "\n",
      "   Cap. Doubtfull it stood,\n",
      "As two spent Swimmers, t\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.raw('shakespeare-macbeth.txt').strip()[:1000]) # First 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e168cb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "1:\t\n",
      "2:\t\n",
      "3:\tActus Primus. Scoena Prima.\n",
      "4:\t\n",
      "5:\tThunder and Lightning. Enter three Witches.\n",
      "6:\t\n",
      "7:\t  1. When shall we three meet againe?\n",
      "8:\tIn Thunder, Lightning, or in Raine?\n",
      "9:\t  2. When the Hurley-burley's done,\n",
      "10:\tWhen the Battaile's lost, and wonne\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(gutenberg.raw('shakespeare-macbeth.txt').split('\\n')):\n",
    "    if i > 10: # Lets take a look at the first 10 lines.\n",
    "        break\n",
    "    print(str(i) + ':\\t' + line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e4205e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Thunder, Lightning, or in Raine?\n"
     ]
    }
   ],
   "source": [
    "macbeth_no8 = gutenberg.raw('shakespeare-macbeth.txt').split('\\n')[8]\n",
    "print(macbeth_no8)#Getting only line 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7bbd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb7b32e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In Thunder, Lightning, or in Raine?']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(macbeth_no8) #the default tokenizer function that you can use to split strings into \"sentences\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a375bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'Thunder', ',', 'Lightning', ',', 'or', 'in', 'Raine', '?']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(macbeth_no8):\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a8e6c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'thunder', ',', 'lightning', ',', 'or', 'in', 'raine', '?']\n"
     ]
    }
   ],
   "source": [
    "#LowerCasing\n",
    "for sent in sent_tokenize(macbeth_no8):\n",
    "    # It's a little in efficient to loop through each word\n",
    "    print([word.lower() for word in word_tokenize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7127ff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'Thunder', ',', 'Lightning', ',', 'or', 'in', 'Raine', '?']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(macbeth_no8))  # Treats the whole line as one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f53e1e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  2.', \"When the Hurley-burley's done,\"]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_no9 = gutenberg.raw('shakespeare-macbeth.txt').split('\\n')[9]\n",
    "sent_tokenize(macbeth_no9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "79b1683d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f292752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e108772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'thunder', ',', 'lightning', ',', 'or', 'in', 'raine', '?']\n"
     ]
    }
   ],
   "source": [
    "# Treat the multiple sentences as one document (no need to sent_tokenize)\n",
    "# Tokenize and lowercase\n",
    "macbeth_no8_tokenized_lowered = list(map(str.lower, word_tokenize(macbeth_no8)))\n",
    "print(macbeth_no8_tokenized_lowered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bcd56133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thunder', ',', 'lightning', ',', 'raine', '?']\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = set(stopwords.words('english')) # Set checking is faster in Python than list.\n",
    "\n",
    "# List comprehension.\n",
    "print([word for word in macbeth_no8_tokenized_lowered if word not in stopwords_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "967eade5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From string.punctuation: <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "# It's a string so we have to them into a set type\n",
    "print('From string.punctuation:', type(punctuation), punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5b2e6824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_', ':', 'there', 'shouldn', 'yourselves', 'and', '=', 'such', 'ma', '&', '\\\\', 'so', '(', 'when', 'have', 'is', 'isn', 'how', 'i', 'm', '+', 'those', \"you'd\", 'during', \"wouldn't\", 'at', 'they', 'until', 'against', 'to', \"you'll\", \"weren't\", 'you', 'needn', 'ourselves', 'them', 'each', 'then', 'nor', 'why', '@', 'itself', 'your', 'who', \"needn't\", 'shan', 'was', '`', 'be', \"shan't\", 'out', 'again', 'over', 'not', \"hadn't\", \"haven't\", 'me', '}', 'this', 'as', 'here', 'most', 'd', 'further', 'these', 'won', 'mustn', 'into', 'himself', 'wouldn', 'doing', \"you're\", 'by', \"'\", 'ours', 'hadn', 'o', 'am', 'own', 'same', 'he', 'having', 'aren', 'through', 'doesn', \"wasn't\", \"it's\", '#', 'haven', 'yourself', 'does', 'hasn', \"mustn't\", '<', ')', 'his', 'now', 'my', 'which', 'been', 'theirs', 'while', 'ain', '>', 'down', 'mightn', \"won't\", 'hers', 'if', 'weren', \"didn't\", 'only', 'll', 'in', 'where', 'couldn', \"that'll\", 'because', 'all', '/', 'wasn', 'under', 'y', 'its', 'their', 'were', 'do', 'very', '$', 'should', 'too', 'before', 't', 'for', 'or', 'whom', 'from', 'had', 'our', 'we', 'above', 'with', 'both', 'don', 'it', '-', ']', 've', 'are', 'some', 'yours', 'the', \"mightn't\", '!', 'about', \"she's\", \"don't\", 's', 'she', ',', 'that', 'being', 'will', ';', '%', 'other', 'myself', 'any', 'few', 'just', 'has', '^', '.', 'no', \"isn't\", '~', 'themselves', 'did', 'than', 'more', 're', 'after', 'up', 'below', '?', \"shouldn't\", '*', 'didn', 'an', 'once', 'between', 'herself', 'what', 'can', '[', '\"', '|', \"aren't\", \"doesn't\", \"hasn't\", 'off', 'on', 'a', 'him', '{', 'her', \"should've\", \"you've\", 'of', \"couldn't\", 'but'}\n"
     ]
    }
   ],
   "source": [
    "#Combining the punctuation with the stopwords from NLTK\n",
    "stopwords_en_withpunct = stopwords_en.union(set(punctuation))\n",
    "print(stopwords_en_withpunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c554787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thunder', 'lightning', 'raine']\n"
     ]
    }
   ],
   "source": [
    "#Removing stopwords with punctuations from Macbeth no. 8\n",
    "print([word for word in macbeth_no8_tokenized_lowered if word not in stopwords_en_withpunct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cac2f8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With combined stopwords:\n",
      "['thunder', 'lightning']\n"
     ]
    }
   ],
   "source": [
    "#combine the stopwords we have in NLTK with other stopwords list we find online.\n",
    "# Stopwords from stopwords-json\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"raine\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\n",
    "\n",
    "# Remove the stopwords from `single_no8`.\n",
    "print('With combined stopwords:')\n",
    "print([word for word in macbeth_no8_tokenized_lowered if word not in stoplist_combined])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca19ef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "walk\n",
      "walk\n"
     ]
    }
   ],
   "source": [
    "#we want to map the different forms \n",
    "#of the same word to the same root word, e.g. \"walks\", \"walking\", \"walked\" should all be the same as \"walk\".\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer() #Trying to shorten a word with simple regex rules\n",
    "\n",
    "for word in ['walking', 'walks', 'walked']:\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d553734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9f6684b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking\n",
      "walk\n",
      "walked\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer() #Trying to find the root word with linguistics rules (with the use of regexes)\n",
    "\n",
    "for word in ['walking', 'walks', 'walked']:\n",
    "    print(wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "300ea86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRP'), ('is', 'VBZ'), ('walking', 'VBG'), ('to', 'TO'), ('school', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#WordNetLemmatizer.lemmatize() function will assume that the word is a Noun if there's no explict POS tag in the input.\n",
    "\n",
    "#First you need the pos_tag function to\n",
    "#tag a sentence and using the tag convert it into WordNet tagsets and then put it through to the WordNetLemmatizer.\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' # if mapping isn't found, fall back to Noun.\n",
    "    \n",
    "# `pos_tag` takes the tokenized sentence as input, i.e. list of string,\n",
    "# and returns a tuple of (word, tg), i.e. list of tuples of strings\n",
    "# so we need to get the tag from the 2nd element.\n",
    "\n",
    "walking_tagged = pos_tag(word_tokenize('He is walking to school'))\n",
    "print(walking_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "38c86188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'be', 'walk', 'to', 'school']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in walking_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "29e28549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'be', 'walk', 'to', 'school']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "lemmatize_sent('He is walking to school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b662ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "single_no8 = webtext.raw('singles.txt').split('\\n')[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "25f7aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Single no. 8:\n",
      "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed. \n",
      "\n",
      "Lemmatized and removed stopwords:\n",
      "['lose', 'r/ship', 'hope', 'sight', 'explore', 'beginning', 'im', 'slim/med', 'build', 'gsoh', 'high', 'similar', 'wont', 'disappoint']\n"
     ]
    }
   ],
   "source": [
    "print('Original Single no. 8:')\n",
    "print(single_no8, '\\n')\n",
    "print('Lemmatized and removed stopwords:')\n",
    "print([word for word in lemmatize_sent(single_no8) \n",
    "       if word not in stoplist_combined\n",
    "       and not word.isdigit() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c2a2c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    return [word for word in lemmatize_sent(text) \n",
    "            if word not in stoplist_combined\n",
    "            and not word.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "65e5847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strings to Vectors\n",
    "#Vector is an array of numbers\n",
    "from collections import Counter\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "# Lemmatize and remove stopwords\n",
    "processed_sent1 = preprocess_text(sent1)\n",
    "processed_sent2 = preprocess_text(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7e29bc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sentence:\n",
      "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
      "\n",
      "Word counts:\n",
      "Counter({'brown': 2, 'quick': 1, 'fox': 1, 'jump': 1, 'lazy': 1, 'dog': 1})\n"
     ]
    }
   ],
   "source": [
    "print('Processed sentence:')\n",
    "print(processed_sent1)\n",
    "print()\n",
    "print('Word counts:')\n",
    "print(Counter(processed_sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75a42882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sentence:\n",
      "['mr', 'brown', 'jump', 'lazy', 'fox']\n",
      "\n",
      "Word counts:\n",
      "Counter({'mr': 1, 'brown': 1, 'jump': 1, 'lazy': 1, 'fox': 1})\n"
     ]
    }
   ],
   "source": [
    "print('Processed sentence:')\n",
    "print(processed_sent2)\n",
    "print()\n",
    "print('Word counts:')\n",
    "print(Counter(processed_sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2fb285d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization with sklearn\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
    "    # Create the vectorizer\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit_transform(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "25ef3da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 8,\n",
       " 'quick': 7,\n",
       " 'brown': 0,\n",
       " 'fox': 2,\n",
       " 'jumps': 3,\n",
       " 'over': 6,\n",
       " 'lazy': 4,\n",
       " 'dog': 1,\n",
       " 'mr': 5}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can check the vocabulary in our vectorizer\n",
    "# It's a dictionary where the words are the keys and \n",
    "# The values are the IDs given to each word. \n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d9c585e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '``', 'ai', 'ca', \"n't\", 'sha', 'wo'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'quick': 6, 'brown': 0, 'fox': 2, 'jumps': 3, 'lazy': 4, 'dog': 1, 'mr': 5}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#override the tokenizer and stop_words\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
    "    # Override the analyzer totally with our preprocess text\n",
    "    count_vect = CountVectorizer(stop_words=stoplist_combined,\n",
    "                                 tokenizer=word_tokenize)\n",
    "    count_vect.fit_transform(fin)\n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f407f38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x7 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.transform([sent1, sent2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c9fc58da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
      "['mr', 'brown', 'jump', 'lazy', 'fox']\n",
      "\n",
      "Vocab: ('brown', 'dog', 'fox', 'jumps', 'lazy', 'mr', 'quick')\n",
      "\n",
      "Matrix/Vectors:\n",
      " [[2 1 1 1 1 0 1]\n",
      " [1 0 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#view the matrix and output it to an array\n",
    "from operator import itemgetter\n",
    "\n",
    "# Print the words sorted by their index\n",
    "words_sorted_by_index, _ = zip(*sorted(count_vect.vocabulary_.items(), key=itemgetter(1)))\n",
    "\n",
    "print(preprocess_text(sent1))\n",
    "print(preprocess_text(sent2))\n",
    "print()\n",
    "print('Vocab:', words_sorted_by_index)\n",
    "print()\n",
    "print('Matrix/Vectors:\\n', count_vect.transform([sent1, sent2]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f2aa5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "url = \"https://raw.githubusercontent.com/marycboardman/Random-Acts-of-Pizza/master/train.json\"\n",
    "response = urlopen(url)\n",
    "trainjson = json.loads(response.read())\n",
    "#with open('../input/random-acts-of-pizza/train.json') as fin:\n",
    "    #trainjson = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "af9728b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 0,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 1,\n",
       " 'post_was_edited': False,\n",
       " 'request_id': 't3_l25d7',\n",
       " 'request_number_of_comments_at_retrieval': 0,\n",
       " 'request_text': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'request_text_edit_aware': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'request_title': 'Request Colorado Springs Help Us Please',\n",
       " 'requester_account_age_in_days_at_request': 0.0,\n",
       " 'requester_account_age_in_days_at_retrieval': 792.4204050925925,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 792.4204050925925,\n",
       " 'requester_number_of_comments_at_request': 0,\n",
       " 'requester_number_of_comments_at_retrieval': 0,\n",
       " 'requester_number_of_comments_in_raop_at_request': 0,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 0,\n",
       " 'requester_number_of_posts_at_request': 0,\n",
       " 'requester_number_of_posts_at_retrieval': 1,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
       " 'requester_number_of_subreddits_at_request': 0,\n",
       " 'requester_received_pizza': False,\n",
       " 'requester_subreddits_at_request': [],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 0,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 1,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 0,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 1,\n",
       " 'requester_user_flair': None,\n",
       " 'requester_username': 'nickylvst',\n",
       " 'unix_timestamp_of_request': 1317852607.0,\n",
       " 'unix_timestamp_of_request_utc': 1317849007.0}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainjson[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "114fd887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID:\t t3_l25d7 \n",
      "\n",
      "Title:\t Request Colorado Springs Help Us Please \n",
      "\n",
      "Text:\t Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated \n",
      "\n",
      "Tag:\t False\n"
     ]
    }
   ],
   "source": [
    "print('UID:\\t', trainjson[0]['request_id'], '\\n')\n",
    "print('Title:\\t', trainjson[0]['request_title'], '\\n')\n",
    "print('Text:\\t', trainjson[0]['request_text_edit_aware'], '\\n')\n",
    "print('Tag:\\t', trainjson[0]['requester_received_pizza'], end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "63487d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneha\\AppData\\Local\\Temp/ipykernel_15880/3275492557.py:3: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  df = pd.io.json.json_normalize(trainjson) # Pandas magic...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_title</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_l25d7</td>\n",
       "      <td>Request Colorado Springs Help Us Please</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_rcb83</td>\n",
       "      <td>[Request] California, No cash and I could use ...</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lpu5j</td>\n",
       "      <td>[Request] Hungry couple in Dundee, Scotland wo...</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_mxvj3</td>\n",
       "      <td>[Request] In Canada (Ontario), just got home f...</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_1i6486</td>\n",
       "      <td>[Request] Old friend coming to visit. Would LO...</td>\n",
       "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id                                      request_title  \\\n",
       "0   t3_l25d7            Request Colorado Springs Help Us Please   \n",
       "1   t3_rcb83  [Request] California, No cash and I could use ...   \n",
       "2   t3_lpu5j  [Request] Hungry couple in Dundee, Scotland wo...   \n",
       "3   t3_mxvj3  [Request] In Canada (Ontario), just got home f...   \n",
       "4  t3_1i6486  [Request] Old friend coming to visit. Would LO...   \n",
       "\n",
       "                             request_text_edit_aware  requester_received_pizza  \n",
       "0  Hi I am in need of food for my 4 children we a...                     False  \n",
       "1  I spent the last money I had on gas today. Im ...                     False  \n",
       "2  My girlfriend decided it would be a good idea ...                     False  \n",
       "3  It's cold, I'n hungry, and to be completely ho...                     False  \n",
       "4  hey guys:\\n I love this sub. I think it's grea...                     False  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting json to pandas DataFrame\n",
    "import pandas as pd\n",
    "df = pd.io.json.json_normalize(trainjson) # Pandas magic... \n",
    "df_train = df[['request_id', 'request_title', \n",
    "               'request_text_edit_aware', \n",
    "               'requester_received_pizza']]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "175c531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training data before vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# It doesn't really matter what the function name is called\n",
    "# but the `train_test_split` is splitting up the data into \n",
    "# 2 parts according to the `test_size` argument you've set.\n",
    "\n",
    "# When we're splitting up the training data, we're spltting up \n",
    "# into train, valid split. The function name is just a name =)\n",
    "train, valid = train_test_split(df_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7271dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the train and validation set\n",
    "# Initialize the vectorizer and \n",
    "# override the analyzer totally with the preprocess_text().\n",
    "# Note: the vectorizer is just an 'empty' object now.\n",
    "count_vect = CountVectorizer(analyzer=preprocess_text)\n",
    "\n",
    "# When we use `CounterVectorizer.fit_transform`,\n",
    "# we essentially create the dictionary and \n",
    "# vectorize our input text at the same time.\n",
    "train_set = count_vect.fit_transform(train['request_text_edit_aware'])\n",
    "train_tags = train['requester_received_pizza']\n",
    "\n",
    "# When vectorizing the validation data, we use `CountVectorizer.transform()`.\n",
    "valid_set = count_vect.transform(valid['request_text_edit_aware'])\n",
    "valid_tags = valid['requester_received_pizza']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8c64e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "url = \"https://raw.githubusercontent.com/marycboardman/Random-Acts-of-Pizza/master/test.json\"\n",
    "response = urlopen(url)\n",
    "testjson = json.loads(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "14712dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneha\\AppData\\Local\\Temp/ipykernel_15880/492275808.py:2: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  df = pd.io.json.json_normalize(testjson) # Pandas magic...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_title</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>[request] pregger gf 95 degree house and no fo...</td>\n",
       "      <td>Hey all! It's about 95 degrees here and our ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>[Request] Lost my job day after labour day, st...</td>\n",
       "      <td>I didn't know a place like this exists! \\n\\nI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>(Request) pizza for my kids please?</td>\n",
       "      <td>Hi Reddit. Im a single dad having a really rou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>[Request] Just moved to a new state(Waltham MA...</td>\n",
       "      <td>Hi I just moved to Waltham MA from my home sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>[Request] Two girls in between paychecks, we'v...</td>\n",
       "      <td>We're just sitting here near indianapolis on o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id                                      request_title  \\\n",
       "0   t3_i8iy4  [request] pregger gf 95 degree house and no fo...   \n",
       "1  t3_1mfqi0  [Request] Lost my job day after labour day, st...   \n",
       "2   t3_lclka                (Request) pizza for my kids please?   \n",
       "3  t3_1jdgdj  [Request] Just moved to a new state(Waltham MA...   \n",
       "4   t3_t2qt4  [Request] Two girls in between paychecks, we'v...   \n",
       "\n",
       "                             request_text_edit_aware  \n",
       "0  Hey all! It's about 95 degrees here and our ki...  \n",
       "1  I didn't know a place like this exists! \\n\\nI ...  \n",
       "2  Hi Reddit. Im a single dad having a really rou...  \n",
       "3  Hi I just moved to Waltham MA from my home sta...  \n",
       "4  We're just sitting here near indianapolis on o...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.io.json.json_normalize(testjson) # Pandas magic... \n",
    "df_test = df[['request_id', 'request_title', \n",
    "               'request_text_edit_aware']]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "59d565ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When vectorizing the test data, we use `CountVectorizer.transform()`.\n",
    "test_set = count_vect.transform(df_test['request_text_edit_aware'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "667d57fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive Bayes classifier in sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB() \n",
    "\n",
    "# To train the classifier, simple do \n",
    "clf.fit(train_set, train_tags) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f71495f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza reception accuracy = 72.77227722772277\n"
     ]
    }
   ],
   "source": [
    "#Checking how good it is on the validation set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# To predict our tags (i.e. whether requesters get their pizza), \n",
    "# we feed the vectorized `test_set` to .predict()\n",
    "predictions_valid = clf.predict(valid_set)\n",
    "\n",
    "print('Pizza reception accuracy = {}'.format(\n",
    "        accuracy_score(predictions_valid, valid_tags) * 100)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b9757919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the full training data set and re-vectorize and retrain the classifier\n",
    "count_vect = CountVectorizer(analyzer=preprocess_text)\n",
    "\n",
    "full_train_set = count_vect.fit_transform(df_train['request_text_edit_aware'])\n",
    "full_tags = df_train['requester_received_pizza']\n",
    "\n",
    "# Note: We have to re-vectorize the test set since\n",
    "#       now our vectorizer is different using the full \n",
    "#       training set.\n",
    "test_set = count_vect.transform(df_test['request_text_edit_aware'])\n",
    "\n",
    "# To train the classifier\n",
    "clf = MultinomialNB() \n",
    "clf.fit(full_train_set, full_tags) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "761620d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To predict our tags (i.e. whether requesters get their pizza), \n",
    "# we feed the vectorized `test_set` to .predict()\n",
    "predictions = clf.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9b522f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 4040 requests, only 994 gets their pizzas, 24.603960396039604% success rate...\n"
     ]
    }
   ],
   "source": [
    "success_rate = sum(df_train['requester_received_pizza']) / len(df_train) * 100\n",
    "print(str('Of {} requests, only {} gets their pizzas,'\n",
    "          ' {}% success rate...'.format(len(df_train), \n",
    "                                        sum(df_train['requester_received_pizza']), \n",
    "                                       success_rate)\n",
    "         )\n",
    "     )\n",
    "#From the training data, we had 24% pizza giving rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b97c5f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 1631 requests, only 55 gets their pizzas, 3.3721643163703248% success rate...\n"
     ]
    }
   ],
   "source": [
    "success_rate = sum(predictions) / len(predictions) * 100\n",
    "print(str('Of {} requests, only {} gets their pizzas,'\n",
    "          ' {}% success rate...'.format(len(predictions), \n",
    "                                        sum(predictions), \n",
    "                                       success_rate)\n",
    "         )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fee54a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id  requester_received_pizza\n",
       "0   t3_i8iy4                         0\n",
       "1  t3_1mfqi0                         0\n",
       "2   t3_lclka                         0\n",
       "3  t3_1jdgdj                         0\n",
       "4   t3_t2qt4                         0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We've kept the `request_id` previous in the `df_test` dataframe.\n",
    "# We can simply merge that column with our predictions.\n",
    "df_output = pd.DataFrame({'request_id': list(df_test['request_id']), \n",
    "                          'requester_received_pizza': list(predictions)}\n",
    "                        )\n",
    "# Convert the predictions from boolean to integer.\n",
    "df_output['requester_received_pizza'] = df_output['requester_received_pizza'].astype(int)\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bf12ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the csv file.\n",
    "df_output.to_csv('basic-nlp-submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d234b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
